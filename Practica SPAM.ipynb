{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correos electronicos  SPAM: Un enfoque con procesamiento de lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ortega Hernandez Jose Manuel 15111304"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los correos electronicos no deseados en su bandeja de entrada son molestos ya que perturban la turina del usuario. Es por eso que las cuentas de correo electronico ya tiene un filtro de spam. Dado que es una de las aplicaciones de pln mas utilizadas vamos a ver como se desarrollo un filtro de spam simple para correos electronicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las famosas librerias\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insertamos los datos \n",
    "full_corpus = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None, names=['label','msg_body'])\n",
    "\n",
    "# Separamos los mensajes 'ham' y 'spam'\n",
    "ham_text = []\n",
    "spam_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams\n",
    "\n",
    "los N-gramos se usan para mdelar el lenguaje en funcion de la prediccion de palabras, es decr, predice la siguiente palabra de una oracion de pallabras N-1 anteriores. Biagram es la secuencia de 2 palabras de N-gramos que predice la siguiente palabra de una racion usando la palabra anterior. En lugar de considerar la historia completa de una oracion o una secuencia de palabras en particular, un modelo como biagram puede ser ocupado en terminos de una aproximacion de la historiaal ocipa una historia limitada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La indentificacion de un mensaje como \"ham\" o como \"spam\". Ademas , hay 5568 mensajes en un DataFrame escrito en ingles que no sn ojetos nulos. por lotanto, el archivo tsv se puede leer usando data frame en python para clasificar esos mensajes de acuerdo con el indicador dador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inspeccion y separacion de mensajes en las categorias \"ham\" y \"spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inicialmente, el conjunto de datos debe inspeccionarse para ociparlo y aordarlo para lograr la tarea. El formato de los datos dados, la canditad de datos proporcionados, la natraleza de los datos se incluyen en esta inspeccion  para indentificar la mejor aproximacion posible para la tarea.\n",
    "\n",
    "\n",
    "El corpus de mensaje dado ha amrcado cada mensaje como ha y spam. Ademas,  Ademas , hay 5568 mensajes en un DataFrame escrito en ingles que no sn ojetos nulos. por lotanto, el archivo tsv se puede leer usando data frame en python para clasificar esos mensajes de acuerdo con el indicador dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_msgs():\n",
    "    for index, column in full_corpus.iterrows():\n",
    "    label = column[0]\n",
    "    message_text = column[1]\n",
    "    if label =='ham':\n",
    "        ham_text.append(message_text)\n",
    "    elif label == 'spam':\n",
    "        spam_text.append(message_text)\n",
    "        \n",
    "separate_msgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de texto\n",
    "\n",
    "\n",
    "el preprosemiento es la tarea de realizar los pasos de preparacin en el corpus de texto sin formato para compeltar de manera eficiente una extraccion de texto o procesamiento de lenguaje natural o cualquier otra tarea que incluya texto sin formato. el preprocesamiento de texto consta de carios pasos, aunque alguno de ellos pueden no aplicarse a una tarea en particular debido a la naturaleza del conjunto de datos disponibles.\n",
    "\n",
    "\n",
    "En esta tarea, el preprocesamiento de texto incluye los siguientes pasos de acuerdo con el conjunto de datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminacion de signos de puntuacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminacion de los signos de puntuacion de los mensajes de correo electronico\n",
    "def remove_msg_punctuations(email_msg):\n",
    "    puntuation_removed_msg = \"\".join([word for word in email_msg if word not in string.punctuation])\n",
    "    return puntuation_removed_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertir a minusculas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir a minusculas: La conversion de todos los caracteres del texto en un contecto comun, como los soportes en minusculas, impide indentificat dos palaras de manera diferente donde una esta en minuscula y a la otra no. Por ejemplo, \"Primero\" y \"Primero\" deben indentificarse como iguales, por lot anto, poner en minuscula todos los caracteres facilita la tarea. Ademas, las palabras de detencion tambien estan en minusculas, por lo que esto tambien haria posible eliminar palabras de deteccion mas adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tokenizacion es la tarea  de dividir el texto en partes significaivas, es decir, tokens que incluyen oraciones y palabras. Un token se puede considerar como una instancia de una secuencia  de caracteres en un texto particular que se agrupan para proporcionar una unidad semantica  util para su posterior procesamiento. En esta tarea, la tokenizacion de palabras se realiza combinando espacios en blanco ente pallabras coo delimitador. Esto se logra en python usando expresiones regulares para diividir una cadena en subcadenas con la funcion split(), que es un tokenizador basico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converti el texto a minusculas y tokenizing de palabras\n",
    "def tokenize_into_words(text):\n",
    "    tokens = re.split('\\W+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palabras lematizantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La derivacion es el proceso de eliminar afijos (sufijos, prefijos, infijos, circunfijos ) de una palabra para obtener su raiz de palabra. Aunque la lematizacion esta relacionada con la derivacion, difiere ya que la lematizacion puede capurar formas canonicas basadas en el lema de una palabra. La lematizacion ocupa un vocavulario y un analisis morfologico de las paabras que lo hacen mas rapido y preciso de la derivacion.\n",
    "WordNetLemmatizerha logrado la lematizacion en lenguaje python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatization(tokenized_words):\n",
    "    lemmatized_tec = [word_lemmatize.lemmatize(word)for worf in tokenized_words]\n",
    "    \n",
    "    \n",
    "    \n",
    "def preprocessing_msgs(corpus):\n",
    "    categorized_text = pd.DataFrame(corpus)\n",
    "    categorized_text['non_punc_message_body'] = categorized_text[0].apply(lambda msg: remove_msg_punctuations(msg))\n",
    "    categorized_text['tokenized_msg_body'] = categorized_text['non_punc_message_body'].apply(lambda msg: tokenize_into_words(msg.lower()))\n",
    "    categorized_text['lemmatized_msg_words'] = categorized_text['tokenized_msg_body'].apply(lambda word_list: lemmatization(word_list))\n",
    "    return categorized_text['lemmatized_msg_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extraccion de caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues de la etapa de preprocesaiento, las caracteristicas debe extrarse del texto. las caracteristicas son las unidades que admiten la tarea de clasificacion, y las biagrams son las caracteristicas en esta tarea de clasificacion de ensajes. Los biagrams o las caracteristicas se extraen de texto preprocesado. Inicialmente, los unigramas se adquieren, y luego esos unigramas , y luego esos unigramas se utilizan para obtener los unigramas en cada corpus(\"ham\" y \"spam\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracion de caracteristicas. Ejemplos: n-grms\n",
    "def feature_extraction(preprocessed_text):\n",
    "    bigrams = []\n",
    "    unigrams_lists = []\n",
    "    for msg in preprocessed_text:\n",
    "        # Agregano end of and starof al mesaje\n",
    "        msg = '<s>' +msg+ '</s>'\n",
    "        unigrams_lists.append(msg.split())\n",
    "unigrams = [uni_list for sub_list in unigrams_lists for uni_list in sub_list]\n",
    "bigrams.extend(nltk.bigrams(unigrams))\n",
    "return bigrams \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Eliminacion de stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay ciertas palabras en un idioma(se utiliza ingles en la practica) que son necesarias para una oracion o una secuencia de paabras, aunque no contribuyen al significado de una frase considerada. La biblioteca del kit de herrramientas del lenguaje natural(NLTK) en python proporciona palabras de deteccion comunes para algunos idiomas.\n",
    "\n",
    "\n",
    "En lugar de elimiinar las palabras de detencion en el paso de preprocesamiento, se realiza despues de extraer las caracteristicas del corpues para evitar la ausencia de biagrams con palabras con palabras de una parada(('use','your'),('to','win')) al adiquiri las funciones, ya que tienen un impacto en el resultado final de la aplicacion. Las palabras de detencion se pueen ignorar en esta recuperacion de informacion orientada a las palabras clabe devido a su efecto en la precision de la recuperacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminacion biagrams solo con stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def filter_stopwords_bigrams(bigrams_list):\n",
    "    filtered_bigrams = []\n",
    "    for bigram in bigram_list:\n",
    "        if bigram[0] in stopwords and bigram[1] in stopwords:\n",
    "            continue\n",
    "        filtered_bigrams.append(bigram)\n",
    "    return filtered_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Obtener distribuion de frecuencia de caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribucion de frecuencia se utiliza  para obtener la frecuencia de aparicion de cada elemento de vocavulario en un texto determinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adquiriendo la frecuencia de caracteristicas\n",
    "def ham_bigram_feature_frequency():\n",
    "    # Frecuencia de caracteristicass para mensajes ham\n",
    "    ham_bigrams = feature_extraction(preprocessing_msgs(ham_text))\n",
    "    ham_biagram_frequency = nltk.FreqDist(filter_stopwords_bigrams(ham_bigrams))\n",
    "    return ham_bigram_frequency\n",
    "\n",
    "def spam_bigram_feature_frequency():\n",
    "    #Frecuencia de caracteristicas para mensajes spam\n",
    "    spam_bigrams = feature_extraction(preprocessing_msgs(spam_text))\n",
    "    spam_bigram_frequency = nltk.FreqDist(filter_stopword_bigrams(spam_bigrams))\n",
    "    return spam_bigram_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Construyendo un modelo para la prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo para clasificar un mensaje dado comom \"ham\" o spam se ha abordado calculando las probabilidades de bigram dentro de cada corpus.Inicialmente, el mensaje dado debe procesarse previamene para avanzar con la clasificacion, incluida la elimincion de signos de puntuacion, el cambio de todos los caracteres a minusculas, la tokenizacion y la lematizacion. Luego, los bigrams se extraen del texto peprocesado para calcular finalmente la probabilidad de que el texto este en cada corpus \"ham\" o \"spam\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando probabilidades del bigram\n",
    "def bigram_probability(message):\n",
    "    probability_h = 1\n",
    "    probability_s = 1\n",
    "    # Preprocesando los mensaje de entrada\n",
    "    punc_removed_message = \"\".join(word for word in message if word not in string.punctuation)\n",
    "    punc_removed_message = '<s> ' +punc_removed_message +' </s>'\n",
    "    tokenized_msg = re.split('\\s+', punc_removed_message)\n",
    "    lemmatized_msg = [word_lemmatizer.lemmatize(word)for word in tokenized_msg]\n",
    "    # bigrams para el mensaje\n",
    "    bigrams_for_msg = list(nltk.bigrams(lemmatized_msg))\n",
    "    # Eliminamos stop words\n",
    "    ham_unigrams = [word for word in feature_extraction(preprocessing_msgs(ham_text)) if word not in stopwords]\n",
    "    spam_unigrams = [word for word in feature_extraction(preprocessing_msgs(spam_text)) if word not in stopwords]\n",
    "    # Frecuencias de bigrams extraidas\n",
    "    ham_frequency = ham_bigram_feature_frequency()\n",
    "    spam_frequency  = spam_bigram_feature_frequency()\n",
    "    print('========================== Calculando Probabilidades ==========================')\n",
    "    print('----------- Frecuencias Ham ------------')\n",
    "    for bigram in bigrams_for_msg:\n",
    "        # probabilidad de la primera palabra en bigram\n",
    "        ham_probability_denominator = 0\n",
    "        # probabilidad de bigram (suavizado) \n",
    "        ham_probability_of_bigram = ham_frequency[bigram] + 1\n",
    "        print(bigram, ' ocurre ', ham_probability_of_bigram)\n",
    "        for (first_unigram, second_unigram) in filter_stopwords_bigrams(ham_unigrams):\n",
    "            ham_probability_denominator += 1\n",
    "            if(first_unigram == bigram[0]):\n",
    "                ham_probability_denominator += ham_frequency[first_unigram, second_unigram]\n",
    "        probability = ham_probability_of_bigram / ham_probability_denominator\n",
    "        probability_h *= probability\n",
    "    print('\\n')\n",
    "    print('----------- Frecuencias Spam ------------')\n",
    "    for bigram in bigrams_for_msg:\n",
    "        # probabilidad de la primera palabra en bigram\n",
    "        spam_probability_denominator = 0\n",
    "        # probabilidad de bigram (suavizado) \n",
    "        spam_probability_of_bigram = spam_frequency[bigram] + 1\n",
    "        print(bigram, ' ocurre ', spam_probability_of_bigram)\n",
    "        for (first_unigram, second_unigram) in filter_stopwords_bigrams(spam_unigrams):\n",
    "            spam_probability_denominator += 1\n",
    "            if(first_unigram == bigram[0]):\n",
    "                spam_probability_denominator += spam_frequency[first_unigram, second_unigram]\n",
    "        probability = spam_probability_of_bigram / spam_probability_denominator\n",
    "        probability_s *= probability\n",
    "    print('\\n')\n",
    "    print('Probabilidad Ham: ' +str(probability_h))\n",
    "    print('Probabildiad Spam: ' +str(probability_s))\n",
    "    print('\\n')\n",
    "    if(probability_h >= probability_s):\n",
    "        print('\\\"' +message +'\\\" es un mensaje Ham')\n",
    "    else:\n",
    "        print('\\\"' +message +'\\\" es un mensaje Spam')\n",
    "    print('\\n')\n",
    "bigram_probability('Click here,  ..to win an iphone 11 pro max')\n",
    "bigram_probability('Win a brand new car ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
